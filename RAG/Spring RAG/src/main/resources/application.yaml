spring:
  application:
    name: Spring RAG
  ai:
    ollama:
      base-url: http://ollama:11434 # Refer to the docker-compose file.
      chat:
        # All properties prefixed with spring.ai.ollama.chat.options can be overridden at runtime
        #   by adding a request specific Runtime Options to the Prompt call.
        options:
          model: llama3.1:8b # 8 billion parameters version.
          temperature: 0.5 # Increasing the temperature will make the model answer more creatively.
          # Making use of GPU (check out the ollama official docker image for further configs)
          # Set according your hardware
          num-gpu: 1
    vectorstore:
      chroma:
        client:
          host: chromadb
          port: 8000 # That's the default one